# Pytorch Quantization Aware Training

_Pre-requisites: PyTorch 1.6.0 or above_

Correspondingly, you might need *Torchaudio 0.6.0 or above_ and _Torchvision 0.7.0 or above_.

This pre-requisite serves mainly for the layer fusion function. Layer fusion is mandatory for quantization aware training -  in most cases, using un-fused model will fail, because there is no quantized layer implementation for a single batch normalization layer.

In this blog post, 







### References

1. [PyTorch Quantization Aware Training, Lei Mao's Log Book](https://leimao.github.io/blog/PyTorch-Quantization-Aware-Training/)
2. [The Official Fuse Modules Recipe](https://pytorch.org/tutorials/recipes/fuse.html#fuse-modules-recipe)
3. [The Official Quantization Doc](https://pytorch.org/docs/stable/quantization.html#module-torch.quantization)
4. [Static Quantization with Eager Mode in PyTorch](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#beta-static-quantization-with-eager-mode-in-pytorch)
5. [Points to Note during QAT](https://pytorch.org/blog/quantization-in-practice/#points-to-note)
