# Pytorch Quantization Aware Training

**Pre-requisites: PyTorch 1.6.0 or 1.7.0**
Correspondingly, you might need **Torchaudio 0.6.0 or 0.7.0** and **Torchvision 0.7.0 or 0.8.0**.

This pre-requisite serves mainly for the layer fusion function. Layer fusion is mandatory for quantization aware training -  in most cases, using un-fused model will fail, because there is no quantized layer implementation for a single batch normalization layer.

In this blog post, 







### References

1. [PyTorch Quantization Aware Training, Lei Mao's Log Book](https://leimao.github.io/blog/PyTorch-Quantization-Aware-Training/)
2. [The Official Fuse Modules Recipe](https://pytorch.org/tutorials/recipes/fuse.html#fuse-modules-recipe)
3. [The Official Quantization Doc](https://pytorch.org/docs/stable/quantization.html#module-torch.quantization)
4. [Static Quantization with Eager Mode in PyTorch](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#beta-static-quantization-with-eager-mode-in-pytorch)

