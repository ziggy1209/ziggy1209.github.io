# Dataset and Dataloader 

> Dataset class in pytorch basically covers the data in a tuple and enables us to access the index of each data. this is necessary to create dataloader class which can be used to shuffle, apply Mini-Batch Gradient Descent and more.

## Custom Dataset

>If I had eight hours to build a machine learning model, I'd spend the first 6 hours preparing my dataset.

>Data preparation is paramount. Before building a model, become one with the data. Ask: What am I trying to do here?

_Source: @mrdbourke Twitter._

[The officical pytorch dataset & dataloader tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) is a good read.
There is also [a nice tutoiral on customising your own torchvision image dataset](https://www.learnpytorch.io
/04_pytorch_custom_datasets/).

I will mainly focus on _torch.utils.data.Dataset_ and _torch.utils.data.Dataloader_, and introduce how they are used in data preparation stage in a more _'my-way'_ way - that is, how I prepare audio fatures for training my speech classification models.

Suppose we already have features extracted and stored piece-wise in `.npy` format. Let's kick off to the start!

**1. Prepare labels**
You can store your lables in `.txt` or `.csv` or `.npy`, whichever you like. Just remember that they need to be in a column, not a row.

```txt
T_1.npy 0
T_2.npy 0
...
F_1.npy 0
F_2.npy 1
...
```
If it's in `.csv` format, the separators would be `,` instead of `\t`.

**2. Customise Dataset**
>A custom `Dataset` class must implement three functions: __init__, __len__, and __getitem__. 
In the implementation below, the _annotation file_ is in `.csv` format and the features in _feat_dir_ are in `.npy` format. If they are in any other format, note that the loading function needs to be changed correspondingly.

```python
import os
import numpy as np

class CustomImageDataset(Dataset):
    def __init__(self, annotations_file, feat_dir):
        self.feat_labels = pd.read_csv(annotations_file)
        self.feat_dir = feat_dir

    def __len__(self):
        return len(self.feat_labels)

    def __getitem__(self, idx):
        feat_path = os.path.join(self.feat_dir, self.feat_labels.iloc[idx, 0])
        feat = np,load(feat_path)
        label = self.feat_labels.iloc[idx, 1]
        return image, label
```
***__init__***
>The __init__ function is run once when instantiating the `Dataset` object.

```python
def __init__(self, annotations_file, feat_dir):
    self.feat_labels = pd.read_csv(annotations_file)
    self.feat_dir = feat_dir
```

***__len__***
>The __len__ function returns the number of samples in our dataset.

```python
def __len__(self):
    return len(self.img_labels)
```

***__getitem__***
The __getitem__ function loads and returns a sample from the dataset at the given index idx. Based on the index, it identifies the feature’s location on disk, retrieves the corresponding label from the csv data in self.feat_labels, and returns the loaded feature and corresponding label in a tuple.

```python
def __getitem__(self, idx):
    feat_path = os.path.join(self.feat_dir, self.feat_labels.iloc[idx, 0])
    feat = np.load(feat_path)
    label = self.feat_labels.iloc[idx, 1]
    return feat, label
```

**3. Preparing your data for training with DataLoaders**
>The `Dataset` retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval.

>`DataLoader` is an iterable that abstracts this complexity for us in an easy API.

```python
from torch.utils.data import DataLoader

train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)
```

In my case, unlike training models with image data. which uses `pandas.read_image` to directly load the images as tensors, the features are loaded via `np.load`. Therefore, an extra step that converts the loaded numpy arrays into tensors is needed.

